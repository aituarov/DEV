{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parsing data from C:\\DEV\\output\\owner_raw.xlsx\n",
      "Parsing finished successfully\n",
      "Translating sheets of downloaded file...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-4-7e8ba91b03bd>:127: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  headers[ind] = headers[ind-1]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2020.4 sheet translated\n",
      "2020.5 sheet translated\n",
      "End.\n"
     ]
    }
   ],
   "source": [
    "from datetime import datetime\n",
    "from google.cloud import translate_v2\n",
    "from lxml import html\n",
    "import os\n",
    "import requests\n",
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import time\n",
    "\n",
    "\n",
    "__author__ = 'aituarov'\n",
    "\n",
    "\n",
    "DB_LOADER_DIR = 'C:\\\\DEV\\\\output\\\\'\n",
    "DATAFILES_DIR = 'C:\\\\DEV\\\\output\\\\'\n",
    "JSON_FILE_DIR = 'C:\\\\DEV\\\\'\n",
    "\n",
    "\n",
    "use_translation_api = False\n",
    "\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = r\"C:\\DEV\\GoogleTranslationAPI\\GoogleCloudKey_AdilbekServiceAccount.json\"\n",
    "translate_client = translate_v2.Client()\n",
    "target_language = 'en'\n",
    "\n",
    "\n",
    "header = {\n",
    "    'Business name_': 'company',\n",
    "    'Hydroelectric power plant_general': 'Hydro',\n",
    "    'Hydroelectric power plant_Pumped': 'Pumped Hydro',\n",
    "    'Thermal power plant_coal': 'coal',\n",
    "    'Thermal power plant_ＬＮＧ': 'LNG',\n",
    "    'Thermal power plant_oil': 'oil',\n",
    "    'Thermal power plant_LPG': 'LPG',\n",
    "    'Thermal power plant_Other gas': 'Gas other',\n",
    "    'Thermal power plant_Cyanogen mixture': 'Bituminous mixture',\n",
    "    'Thermal power plant_Other': 'Thermal other',\n",
    "    'Nuclear power plant_': 'Nuclear',\n",
    "    'New energy power plant_Wind force': 'Wind',\n",
    "    'New energy power plant_sunshine': 'Solar',\n",
    "    'New energy power plant_Geothermal': 'Geothermal',\n",
    "    'New energy power plant_biomass':'Biomass',\n",
    "    'New energy power plant_waste':'Waste',\n",
    "    'Other_':'Other'   \n",
    "}\n",
    "\n",
    "\n",
    "def download_file():\n",
    "    url = 'https://www.enecho.meti.go.jp/statistics/electric_power/ep002/'\n",
    "    response = requests.get(url + 'results.html')\n",
    "    print(\"Trying to find file from \" + url + 'resulst.html')\n",
    "    page = html.fromstring(response.text)\n",
    "    \n",
    "    files = page.xpath('//div[@class=\"tileListCol1st h15em\"]/ul/li/a')\n",
    "    for file in files:\n",
    "        if file.text.startswith('2-(1)'):\n",
    "            url = url + file.get('href')\n",
    "    response = requests.get(url)\n",
    "    print(\"Starting download file from \" + url)\n",
    "    file_name = DATAFILES_DIR + 'owner_raw.xlsx'\n",
    "    with open(file_name, 'wb') as output:\n",
    "        output.write(response.content)\n",
    "        output.close()\n",
    "    print(\"File downloaded\")\n",
    "    \n",
    "    \n",
    "def parse_needed_data_from_xl(file_name):\n",
    "    print(\"Parsing data from \" + file_name)\n",
    "    xl = pd.ExcelFile(file_name)\n",
    "    needed_sheets_names = []\n",
    "    for sh_name in xl.sheet_names:\n",
    "        if re.search(r'\\d{4}.\\d+', sh_name):\n",
    "            needed_sheets_names.append(sh_name)\n",
    "\n",
    "    raw_data = xl.parse(needed_sheets_names)\n",
    "    print(\"Parsing finished successfully\")\n",
    "    return raw_data\n",
    "    \n",
    "    \n",
    "def update_json(raw_data):\n",
    "    print(\"Starting to update json file\")\n",
    "    if os.path.exists(JSON_FILE_DIR + 'google_translate_v2.json'):\n",
    "        print(\"Find google_translate_v2.json in \" + JSON_FILE_DIR)\n",
    "        with open(JSON_FILE_DIR + 'google_translate_v2.json') as j:\n",
    "            myDict = json.load(j)\n",
    "    else:\n",
    "        print(\"Can't find google_translate_v2.json file in \" + JSON_FILE_DIR + '\\nCreating new json file...')\n",
    "        myDict = {}\n",
    "    \n",
    "    for key in raw_data.keys():\n",
    "        for row in range(raw_data[key].shape[0]):\n",
    "            for col in range(raw_data[key].shape[1]):\n",
    "                if(isinstance(raw_data[key].iloc[row, col], str) and raw_data[key].iloc[row, col] not in myDict):\n",
    "                    new_string = raw_data[key].iloc[row, col]\n",
    "                    myDict[new_string] = translate_client.translate(\n",
    "                        new_string,\n",
    "                        target_language=target_language\n",
    "                        )[\"translatedText\"]           \n",
    "    \n",
    "    with open(JSON_FILE_DIR + 'google_translate_v2.json', \"w\") as outfile:\n",
    "        json.dump(myDict, outfile)\n",
    "    print(\"JSON file successfully updated.\")\n",
    "        \n",
    "    \n",
    "def translate_text(str):\n",
    "    if os.path.exists(JSON_FILE_DIR + 'google_translate_v2.json'):\n",
    "        with open(JSON_FILE_DIR + 'google_translate_v2.json') as j:\n",
    "            dic = json.load(j)\n",
    "\n",
    "        for d in dic:\n",
    "            str = str.replace(d,dic[d])\n",
    "    else:\n",
    "        str = 'Json file not find, please check the json path'\n",
    "    \n",
    "    return str\n",
    "    \n",
    "    \n",
    "def translate_file(raw_data):\n",
    "    print(\"Translating sheets of downloaded file...\")\n",
    "    data_table = pd.DataFrame()\n",
    "\n",
    "    data = {}\n",
    "\n",
    "    for key in raw_data.keys():\n",
    "        data[key] = translate_text(raw_data[key])\n",
    "        headers = data[key].iloc[0]\n",
    "        for ind in range(len(headers)):\n",
    "            if isinstance(headers[ind], float):\n",
    "                headers[ind] = headers[ind-1]\n",
    "    \n",
    "        sub_headers = data[key].iloc[1].replace(np.nan, '')\n",
    "        headers = headers + '_' + sub_headers\n",
    "        data[key].columns = headers\n",
    "        data[key] = data[key].rename(columns=lambda x: header[x] if x in header.keys() else np.nan)\n",
    "        data[key] = data[key].loc[data[key]['company'].notnull(), data[key].columns.notnull()].reset_index(drop=True).head(-1).tail(-1)\n",
    "        \n",
    "        data[key] = pd.melt(data[key], id_vars=['company'], value_vars = list(data[key].columns)[1:], var_name='name', value_name='value')\n",
    "        data[key] = change_format_df(data[key], key)\n",
    "        data_table = data_table.append(data[key])\n",
    "        print(key + \" sheet translated\")\n",
    "        \n",
    "        \n",
    "    data_table.to_csv(DB_LOADER_DIR + \"owner_data.csv\", sep=';',index=False)\n",
    "    print(\"End.\")\n",
    "\n",
    "\n",
    "def change_format_df(data, key):\n",
    "    data['name'] = data['company'].replace('/', '').replace('㈱Global New Energy Togo', 'Global New Energy Togo') + '_' + data['name']\n",
    "    data['effective_date'] = \"\"\n",
    "    data['start_date'] = datetime.strptime(key, '%Y.%m').strftime('%Y-%m-%d')\n",
    "    data['end_date'] = \"\"\n",
    "    data['value'] = data['value'].map(lambda x:round(x) if not (pd.isna(x) or isinstance(x, str)) else float('nan'))\n",
    "    data['table'] = 'spot'\n",
    "    data = data[[\"name\",\"effective_date\",\"start_date\",\"end_date\",\"value\",\"table\"]]\n",
    "    \n",
    "    return data\n",
    "\n",
    "   \n",
    "def main():\n",
    "    download_file()\n",
    "    time.sleep(3)\n",
    "    \n",
    "    raw_data = parse_needed_data_from_xl(DATAFILES_DIR + 'owner_raw.xlsx')\n",
    "    time.sleep(3)\n",
    "    \n",
    "    if use_translation_api:\n",
    "        update_json(raw_data)\n",
    "        time.sleep(3)\n",
    "        \n",
    "    translate_file(raw_data)\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'1': '2', '3': '4'}\n",
      "<class 'dict'>\n",
      "224456\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import json\n",
    "\n",
    "text = '123456'\n",
    "\n",
    "file = 'C:\\\\DEV\\\\output\\\\test.json'\n",
    "\n",
    " \n",
    "\n",
    "def translate(text):\n",
    "    with open(file) as file_r:\n",
    "        dic = json.load(file_r)\n",
    "        print(dic)\n",
    "        print(type(dic))\n",
    "\n",
    "        for d in dic:\n",
    "            text = text.replace(d, dic[d])\n",
    "            \n",
    "    return text\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    print(translate(text))\n",
    "\n",
    "# dic = json.loads(file)\n",
    "# print(dic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Hello\n"
     ]
    }
   ],
   "source": [
    "print('\\nHello')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
