{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.chrome.options import Options\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.common.by import By\n",
    "from datetime import datetime\n",
    "\n",
    "import time\n",
    "\n",
    "chrome_options = Options()\n",
    "driver = webdriver.Chrome(options = chrome_options)\n",
    "\n",
    "\n",
    "__author__ = 'aituarov'\n",
    "\n",
    "\n",
    "DATAFILES_DIR = 'C:\\\\DEV\\\\REMIT_files\\\\'\n",
    "\n",
    "url = 'https://www.elia.be/en/grid-data/power-generation/planned-and-unplanned-outages'\n",
    "\n",
    "def download_snapshots():\n",
    "    driver.get(url)\n",
    "    WebDriverWait(driver,1000).until(EC.visibility_of_all_elements_located((By.XPATH, '//div[@class=\"k-grid k-widget k-display-block\"]')))\n",
    "    time.sleep(2)\n",
    "    \n",
    "    table_divs = driver.find_elements_by_xpath('//div[@class=\"k-grid k-widget k-display-block\"]')\n",
    "    for table_id, table in enumerate(table_divs):\n",
    "        snapshot_fname = str(table_id) + '__' + datetime.today().strftime('%Y%m%d') + '.html'\n",
    "        with open(DATAFILES_DIR + snapshot_fname, 'w') as snapshot_file:\n",
    "            snapshot_file.write(table.get_attribute('outerHTML'))\n",
    "            print(snapshot_fname + \" downloaded.\")\n",
    "            snapshot_file.close()\n",
    "            \n",
    "    driver.quit()\n",
    "            \n",
    "\n",
    "def main():\n",
    "    download_snapshots()\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "File with file_id 0 from C:\\DEV\\REMIT_UNEMPTY\\ is converted to df\n",
      "File with file_id 1 from C:\\DEV\\REMIT_UNEMPTY\\ is converted to df\n",
      "Key is :0\n",
      "       0    1                   Unit   Fuel Pmax Available Pmax Available   \\\n",
      "0  ADDED  1.0                    NaN    NaN            NaN             NaN   \n",
      "2    NaN  NaN          DROGENBOS GT1     NG            150               0   \n",
      "0  ADDED  2.0                    NaN    NaN            NaN             NaN   \n",
      "1    NaN  NaN           DROGENBOS ST     NG            160               0   \n",
      "0  ADDED  3.0  Zelzate 2 Knippegroen  Other            315               0   \n",
      "3    NaN  NaN  Zelzate 2 Knippegroen  Other            315               0   \n",
      "\n",
      "       Start Outage  (estimated) End       Last Updated                Reason  \n",
      "0               NaN               NaN               NaN                   NaN  \n",
      "2  29/07/2020 08:10  29/08/2020 08:10  29/07/2020 08:11              Overhaul  \n",
      "0               NaN               NaN               NaN                   NaN  \n",
      "1  29/07/2020 08:10  24/09/2020 08:10  29/07/2020 08:11              Overhaul  \n",
      "0  14/08/2020 07:31  15/08/2020 07:26  15/08/2020 07:26  Awaiting information  \n",
      "3  14/08/2020 00:00  15/08/2020 07:26  15/08/2020 07:26  Awaiting information  \n",
      "File with key 0 downloaded to C:\\DEV\\REMIT_files\\\n",
      "Key is :1\n",
      "         0     1                   Unit   Fuel Pmax Available Pmax Available   \\\n",
      "0    ADDED   1.0                    NaN    NaN            NaN             NaN   \n",
      "23     NaN   NaN       Amercoeur 1 R GT     NG            289               0   \n",
      "51     NaN   NaN       Amercoeur 1 R GT     NG            289               0   \n",
      "71     NaN   NaN       Amercoeur 1 R GT     NG            289               0   \n",
      "182    NaN   NaN       Amercoeur 1 R GT     NG            289               0   \n",
      "..     ...   ...                    ...    ...            ...             ...   \n",
      "62     NaN   NaN        Zandvliet Power     NG          386.2               0   \n",
      "232    NaN   NaN        Zandvliet Power     NG          386.2               0   \n",
      "0    ADDED  31.0                    NaN    NaN            NaN             NaN   \n",
      "6      NaN   NaN  Zelzate 2 Knippegroen  Other            315               0   \n",
      "60     NaN   NaN  Zelzate 2 Knippegroen  Other            315               0   \n",
      "\n",
      "         Start Outage  (estimated) End       Last Updated             Reason  \n",
      "0                 NaN               NaN               NaN                NaN  \n",
      "23   01/05/2023 00:00  15/05/2023 00:00  12/05/2020 14:29           Overhaul  \n",
      "51   19/03/2022 00:00  23/05/2022 00:00  05/05/2020 11:40           Overhaul  \n",
      "71   01/05/2021 00:00  22/05/2021 00:00  12/05/2020 14:30           Overhaul  \n",
      "182  11/11/2020 00:00  16/11/2020 00:00  10/08/2020 11:39  Technical failure  \n",
      "..                ...               ...               ...                ...  \n",
      "62   12/08/2021 00:01  17/10/2021 23:59  19/05/2020 13:44           Overhaul  \n",
      "232  18/09/2020 00:00  30/09/2020 00:00  19/05/2020 13:51           Overhaul  \n",
      "0                 NaN               NaN               NaN                NaN  \n",
      "6    26/09/2023 00:00  16/10/2023 00:00  29/05/2020 09:52           Overhaul  \n",
      "60   02/09/2021 00:00  29/10/2021 00:00  15/06/2020 09:56           Overhaul  \n",
      "\n",
      "[313 rows x 10 columns]\n",
      "File with key 1 downloaded to C:\\DEV\\REMIT_files\\\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "\n",
    "REMIT_files = 'C:\\\\DEV\\\\REMIT_files\\\\'\n",
    "DATAFILES_DIR = 'C:\\\\DEV\\\\REMIT_files\\\\'\n",
    "\n",
    "REMIT_EMPTY = 'C:\\\\DEV\\\\REMIT_EMPTY\\\\'\n",
    "REMIT_UNEMPTY = 'C:\\\\DEV\\\\REMIT_UNEMPTY\\\\'\n",
    "\n",
    "def convert_html_files_to_df(folder):\n",
    "    data = {}\n",
    "\n",
    "    if not os.listdir(folder):\n",
    "        print(\"There's no files\")\n",
    "    else:\n",
    "        for file_name in os.listdir(folder):\n",
    "            if file_name.endswith('html'):\n",
    "                file_type = int(file_name[0])\n",
    "                data_table = pd.DataFrame()\n",
    "                with open(folder + file_name, 'r') as snapshot_file:\n",
    "                    html_content = snapshot_file.read()\n",
    "                    page = html.fromstring(html_content)\n",
    "                    header = [th.text for th in page.xpath('./div//tr/th')[1:]]\n",
    "\n",
    "                    content_row = page.xpath('./div//tbody/tr')\n",
    "                    for row in content_row:\n",
    "                        row_data = [td.text for td in row.xpath('./td')]\n",
    "\n",
    "                        data_table = data_table.append([row_data], ignore_index=True)\n",
    "\n",
    "                    data_table.columns = header\n",
    "                    data[file_type] = data_table\n",
    "                    print(\"File with file_id \" + str(file_type) + \" from \" + folder + \" is converted to df\")\n",
    "                    snapshot_file.close()\n",
    "                    \n",
    "    return data\n",
    "\n",
    "\n",
    "def compare_df(old_data, new_data):\n",
    "    comparison = {}\n",
    "    \n",
    "    old_uniq_data = {}\n",
    "    new_uniq_data = {}\n",
    "    \n",
    "    for key in new_data.keys():\n",
    "        data = pd.DataFrame()\n",
    "        \n",
    "        if key not in old_data.keys():\n",
    "                old_data[key] = pd.DataFrame(columns=new_data[key].columns)\n",
    "        \n",
    "        old_uniq_data[key]=pd.merge(old_data[key], new_data[key], indicator=True, how='outer').query('_merge==\"left_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "        new_uniq_data[key]=pd.merge(old_data[key],new_data[key], indicator=True, how='outer').query('_merge==\"right_only\"').drop('_merge', axis=1).reset_index(drop=True)\n",
    "        \n",
    "        old_cnt = old_uniq_data[key].groupby(['Unit']).size().reset_index(name='counts')\n",
    "        new_cnt = new_uniq_data[key].groupby(['Unit']).size().reset_index(name='counts')\n",
    "\n",
    "        merged_df=old_cnt.merge(new_cnt, how='outer', on='Unit')\n",
    "        \n",
    "        comp_id = 1\n",
    "        for index, row in merged_df.iterrows():\n",
    "            df_old_match = old_uniq_data[key].loc[old_uniq_data[key]['Unit'] == row['Unit']]\n",
    "            df_new_match = new_uniq_data[key].loc[new_uniq_data[key]['Unit'] == row['Unit']]\n",
    "\n",
    "            if row['counts_x'] == row['counts_y'] == 1:\n",
    "                df_stat = pd.DataFrame([['UPDATED', compid]])\n",
    "                data = data.append(pd.concat([pd.concat([df_stat, df_old_match], axis=1), pd.concat([df_stat, df_new_match], axis=1)], axis=0))\n",
    "\n",
    "            elif row['counts_x'] >= 1 and pd.isna(row['counts_y']):\n",
    "                df_stat = pd.DataFrame([['REMOVED', comp_id]])\n",
    "                data = data.append(pd.concat([df_stat, df_old_match], axis=1))\n",
    "            \n",
    "            elif pd.isna(row['counts_x']) and row['counts_y']>=1:\n",
    "                df_stat = pd.DataFrame([['ADDED', comp_id]])\n",
    "                data = data.append(pd.concat([df_stat, df_new_match], axis=1))\n",
    "            \n",
    "            else:\n",
    "                df_stat = pd.DataFrame([[float('NaN'), float('NaN')]])\n",
    "                data = data.append(pd.concat([df_stat, df_new_match], axis=1))\n",
    "\n",
    "            comp_id+=1\n",
    "        \n",
    "        comparison[key] = data\n",
    "        \n",
    "    return comparison\n",
    "\n",
    "\n",
    "def main():\n",
    "    old_data = convert_html_files_to_df(REMIT_files)\n",
    "    new_data = convert_html_files_to_df(REMIT_UNEMPTY)\n",
    "\n",
    "    comparison = compare_df(old_data, new_data)\n",
    "    for key in comparison.keys():\n",
    "        print(\"Key is :\" + str(key))\n",
    "        print(comparison[key])\n",
    "        \n",
    "        if not comparison[key].empty:\n",
    "            for file_name in os.listdir(REMIT_files):\n",
    "                if file_name.startswith(str(key)):\n",
    "                    os.remove(REMIT_files + file_name)\n",
    "            \n",
    "            for new_file_name in os.listdir(REMIT_UNEMPTY):\n",
    "                if new_file_name.startswith(str(key)):\n",
    "                    shutil.copyfile(REMIT_UNEMPTY + new_file_name, REMIT_files + new_file_name)\n",
    "                    shutil.copyfile(REMIT_UNEMPTY + new_file_name, REMIT_files+\"ARCHIVE\\\\\" + new_file_name)\n",
    "                    \n",
    "            print(\"File with key \" + str(key) + \" downloaded to \" + REMIT_files)\n",
    "        \n",
    "        else:\n",
    "            for file_name in os.listdir(REMIT_UNEMPTY):\n",
    "                if file_name.startswith(str(key)):\n",
    "                    os.remove(REMIT_UNEMPTY + file_name)\n",
    "                    print(\"File with key \" + str(key) + \" removed\")\n",
    "                    \n",
    "\n",
    "\n",
    "\n",
    "if __name__ == '__main__':\n",
    "\n",
    "    main()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "from lxml import html\n",
    "\n",
    "REMIT_files = 'C:\\\\DEV\\\\REMIT_files\\\\'\n",
    "DATAFILES_DIR = 'C:\\\\DEV\\\\REMIT_files\\\\'\n",
    "\n",
    "REMIT_EMPTY = 'C:\\\\DEV\\\\REMIT_EMPTY\\\\'\n",
    "REMIT_UNEMPTY = 'C:\\\\DEV\\\\REMIT_UNEMPTY\\\\'\n",
    "\n",
    "def convert_html_files_to_df(folder):\n",
    "    data = {}\n",
    "\n",
    "    if not os.listdir(folder):\n",
    "        print(\"There's no files\")\n",
    "    else:\n",
    "        for file in os.listdir(folder):\n",
    "            if file.endswith('html'):\n",
    "                file_type = int(file[0])\n",
    "                data_table = pd.DataFrame()\n",
    "                with open(file, 'r') as snapshot_file:\n",
    "                    html_content = snapshot_file.read()\n",
    "                    page = html.fromstring(html_content)\n",
    "                    header = [th.text for th in page.xpath('./div//tr/th')[1:]]\n",
    "                    content_row = page.xpath('./div//tbody/tr')\n",
    "                    for row in content_row:\n",
    "                        row_data = [td.text for td in row.xpath('./td')]\n",
    "                        data_table = data_table.append([row_data], ignore_index=True)\n",
    "\n",
    "                    data_table.columns = header\n",
    "                    data[file_type] = data_table\n",
    "                    print(\"File with file_id \" + str(file_type) + \" is converted to df\")\n",
    "                    snapshot_file.close()\n",
    "                    \n",
    "    return data\n",
    "\n",
    "\n",
    "def main():\n",
    "    old_data = convert_html_files_to_df(REMIT_files)\n",
    "    print(old_data)\n",
    "    new_data = convert_html_files_to_df(REMIT_UNEMPTY)\n",
    "    print(new_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  0  1\n",
      "0  5  6  7  8\n",
      "0  5  6  5  6\n"
     ]
    }
   ],
   "source": [
    "# a = pd.DataFrame([[1, 2, 3, 4], [1, 2, 3, 4]])\n",
    "# b = pd.DataFrame([34, 45])\n",
    "# c = a.loc[a[0] == 1]\n",
    "# for row in c[:]:\n",
    "#     print(row)\n",
    "    \n",
    "a = pd.DataFrame([[1, 2, 3, 4], [1, 2, 3, 4]])\n",
    "b = pd.DataFrame([[5, 6]])\n",
    "c = pd.DataFrame([[7, 8]])\n",
    "e = pd.DataFrame([[5, 6]])\n",
    "d = pd.DataFrame()\n",
    "\n",
    "# print(a)\n",
    "# print(b)\n",
    "\n",
    "# b = b.loc[b.index.repeat(2)].reset_index(drop=True)\n",
    "# print(b)\n",
    "\n",
    "# print(pd.concat([a, b], axis=1))\\\n",
    "\n",
    "d = d.append(pd.concat([pd.concat([b, c], axis=1), pd.concat([b, e], axis=1)], axis=0))\n",
    "print(d)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "metadata": {},
   "outputs": [],
   "source": [
    "dicti = {}\n",
    "dicti[1] = 'sadfasdf'\n",
    "if 2 in dicti.keys():\n",
    "    print('OK')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
